"""
Domain Generation Algorithm (DGA) Detector using CNN
----------------------------------------------------

This script trains a Convolutional Neural Network (CNN) to detect malicious
domains generated by various malware families (DGA domains). It combines
legitimate domains with labeled malicious datasets (e.g., Conficker, Zeus,
Cryptolocker) and encodes domain names into character-level embeddings.

Key Features:
- Loads and merges domain datasets (`dataset_all.csv`, `tranco_5XY6N_full_list.csv`)
- Maps domain family names to numeric labels (e.g., Conficker=1, Zeus=3, etc.)
- Converts domain strings into fixed-length integer sequences (character indexing)
- Uses stratified train/test split to preserve class balance
- Builds a CNN model: Embedding → Conv1D → GlobalMaxPooling → Dense → Dropout
- Handles class imbalance with automatic class weights
- Evaluates performance with Accuracy, Precision, Recall, Classification Report, and Confusion Matrix
- Measures inference latency (ms per sample)

Output:
- Trained CNN model (in memory, not saved to disk by default)
- Console logs with training progress, metrics, predictions, and confusion matrix
"""

import pandas as pd
import numpy as np
import time
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical


# 1. Data Loading & Preprocessing
column_names = ['index', 'domain', 'label']
df1 = pd.read_csv('/Users/florence/Desktop/dataset_all.csv', header=None, names=column_names)
df2 = pd.read_csv('/Users/florence/Desktop/domain/tranco_5XY6N_full_list.csv', header=None, names=column_names)
df2['label'] = 'legit'
df = pd.concat([df1, df2], axis=0)
print(type(df))

# Label encoding (maps text labels to integers for compatibility)
label_mapping = {
    'legit': 0,
    'conficker': 1,      # worm
    'cryptolocker': 2,   # ransomware
    'zeus': 3,           # banking trojan
    'pushdo': 4,         # spambot / botnet
    'rovnix': 5,         # trojan targeting financial institutions
    'tinba': 6,          # tiny banking trojan
    'matsnu': 7,         # backdoor trojan
    'ramdo': 8           # banking trojan variant, similar to rovnix
}

print(df.columns)
df['label'] = df['label'].map(label_mapping)
y = df['label'].values
num_classes = len(df['label'].unique())


# 2. Feature Engineering
chars = 'abcdefghijklmnopqrstuvwxyz0123456789.-'
char_to_idx = {c: i+1 for i, c in enumerate(chars)}  # 0 reserved for padding
max_len = 50  # standardize domain length

def domain_to_vector(domain):
    """Convert a single domain into a sequence of integers (character-level index).
       Filters out any invalid characters automatically.
    """
    return [char_to_idx.get(c, 0) for c in str(domain).lower() if c in char_to_idx]

# Generate feature matrix
X = pad_sequences(
    [domain_to_vector(d) for d in df['domain']],
    maxlen=max_len,
    padding='post',
    truncating='post',
    dtype='int32'
)

# 3. Train-Test Split (80% training, 20% testing, stratified to preserve label proportions)
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print(f"\nTraining set shape: {X_train.shape}, Test set shape: {X_test.shape}")

y_train_onehot = to_categorical(y_train)
y_test_onehot = to_categorical(y_test)


# 4. Build CNN Model
model = Sequential([
    Embedding(input_dim=len(char_to_idx)+1, output_dim=32),  # embedding layer, adapts to input length
    Conv1D(filters=64, kernel_size=3, activation='relu'),
    GlobalMaxPooling1D(),
    Dense(64, activation='relu'),
    Dropout(0.5),  # prevent overfitting
    Dense(9, activation='softmax')
])

# Compute class weights (to handle imbalanced dataset)
class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)
class_weight = {i: class_weights[i] for i in range(len(class_weights))}
print("Automatically computed class weights:", class_weight)


model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['Accuracy', 'Precision', 'Recall']
)


# 5. Train the Model
print("\nStarting training...")
history = model.fit(
    X_train, y_train_onehot,
    validation_data=(X_test, y_test_onehot),
    epochs=10,  # number of epochs
    batch_size=64,
    class_weight=class_weight,  # apply class weights
    verbose=1
)


# 6. Evaluate the Model
print("\n=== Test Set Evaluation ===")
test_loss, test_acc, test_precision, test_recall = model.evaluate(X_test, y_test_onehot, verbose=0)
print(f"Accuracy: {test_acc:.4f} | Precision: {test_precision:.4f} | Recall: {test_recall:.4f}")

# Generate predictions
start_time = time.time()
y_pred = np.argmax(model.predict(X_test), axis=1)
y_true = np.argmax(y_test_onehot, axis=1)
total_time = time.time() - start_time
ms_per_sample = total_time / len(X_test)
print(f"\nInference time per sample: {ms_per_sample:.10f} ms")


print("\n=== Detailed Classification Report ===")
y_pred_onehot = to_categorical(y_pred, num_classes=9)
target_names = label_mapping.keys()
print(classification_report(y_test_onehot, y_pred_onehot, target_names=target_names))

y_true_classes = np.argmax(y_test_onehot, axis=1)
y_pred_classes = np.argmax(y_pred_onehot, axis=1)

print("\n=== Confusion Matrix ===")
print(confusion_matrix(y_true_classes, y_pred_classes))
